'use client';

import locationTracker from '@/hooks/locationTracker';
import { OCRResponse } from '@/types/ocr';
import LABELS from '@app-datasets/coco/classes.json';
import { useQuery } from '@tanstack/react-query';
import * as tf from '@tensorflow/tfjs';
import '@tensorflow/tfjs-backend-webgl';
import { useCallback, useEffect, useRef, useState } from 'react';
import Webcam from 'react-webcam';
import { BusArrivalResult, getBusArrival } from '../api/getBusArrival';
import { getBusNumber } from '../api/userUtils';

export default function Camera() {
  const [hasPermission, setHasPermission] = useState<boolean | null>(null);
  const webcamRef = useRef<Webcam>(null);
  const captureInterval = useRef<NodeJS.Timeout | null>(null);

  const requestCameraPermission = () => {
    setHasPermission(null);

    navigator.mediaDevices
      .getUserMedia({ video: true })
      .then(() => setHasPermission(true))
      .catch(() => setHasPermission(false));
  };

  //ai ìƒíƒœ ê´€ë¦¬ ë° ì°¸ì¡° ë³€ìˆ˜
  const modelRef = useRef<tf.GraphModel | null>(null); // useRefë¡œ ì°¸ì¡°í•˜ê²Œ ìˆ˜ì •
  const [model, setModel] = useState<tf.GraphModel | null>(null);
  const ZOO_MODEL = [{ name: 'yolov5', child: ['yolov5n', 'yolov5s'] }];
  const [modelName] = useState(ZOO_MODEL[0]);
  const [loading, setLoading] = useState(0);
  const [detectedBus, setDetectedBus] = useState<string | null>(null);
  const [isDetectedBusArriving, setIsDetectedBusArriving] = useState(false);
  const [showNotification, setShowNotification] = useState(false);

  const canvasRef = useRef<HTMLCanvasElement>(null);

  useEffect(() => {
    const initializeApp = async () => {
      if (!locationTracker.isTracking()) {
        console.log('ìœ„ì¹˜ ì¶”ì  ì‹œì‘...');
        locationTracker.startTracking();
      }
    };

    initializeApp();

    return () => {
      if (captureInterval.current) {
        clearInterval(captureInterval.current);
      }
    };
  }, []);

  const { data: locationStatus } = useQuery({
    queryKey: ['locationStatus'],
    queryFn: async () => {
      try {
        if (locationTracker.hasNearbyBusStops()) {
          await getBusArrival();
          return true;
        }
        return false;
      } catch (error) {
        console.log('ìœ„ì¹˜ ì •ë³´ ì¤€ë¹„ ì¤‘...', error);
        throw error;
      }
    },
    refetchInterval: (data) => {
      return data ? false : 3000;
    },
    retry: (failureCount) => {
      return failureCount < 20;
    },
    retryDelay: 3000,
    enabled: true,
  });

  const { data: busArrivalData } = useQuery<BusArrivalResult>({
    queryKey: ['busArrivals'],
    queryFn: getBusArrival,
    refetchInterval: 30000,
    enabled: true,
    retry: (failureCount) => {
      return failureCount < 10;
    },
    retryDelay: 1000,
    staleTime: 0,
    gcTime: 0,
  });

  const expectedBuses = busArrivalData?.buses || [];
  const hasNearbyStops = busArrivalData?.hasNearbyStops ?? false;
  const isRegisteredBusArriving = busArrivalData?.isRegisteredBusArriving ?? false;

  console.log('ğŸ”„ Query ìƒíƒœ:', {
    locationStatus,
    hasNearbyBusStops: locationTracker.hasNearbyBusStops(),
    busArrivalData,
    expectedBuses,
    hasNearbyStops,
    isRegisteredBusArriving,
  });

  //ai ëª¨ë¸ ë¡œë”© í•¨ìˆ˜
  useEffect(() => {
    let isMounted = true;
    let loadedModel: tf.GraphModel | null = null;

    const loadModel = async () => {
      try {
        console.log('Starting model load...');
        // Use relative path for model
        const modelPath = `/model/${modelName.name}/${modelName.child[1]}/model.json`;
        console.log('Model path:', modelPath);

        // Check if model files exist
        try {
          const response = await fetch(modelPath);
          if (!response.ok) {
            throw new Error(`Model file not found: ${response.status}`);
          }
          console.log('Model file exists, starting to load...');
        } catch (error) {
          console.error('Model file check failed:', error);
          throw error;
        }

        // Dispose previous model if exists
        if (modelRef.current) {
          // ìˆ˜ì •: model ëŒ€ì‹  modelRef.current ì‚¬ìš©
          console.log('Disposing previous model...');
          modelRef.current.dispose();
        }

        // Set loading state to indicate start
        if (isMounted) {
          setLoading(0.1);
        }

        loadedModel = await tf.loadGraphModel(modelPath, {
          onProgress: (fractions) => {
            console.log('Loading progress:', fractions);
            if (isMounted) {
              setLoading(fractions);
            }
          },
        });

        if (!loadedModel) {
          throw new Error('Model loading failed');
        }

        if (isMounted) {
          console.log('Model loaded, warming up...');
          const shape = loadedModel.inputs[0]?.shape || [1, 640, 640, 3];
          console.log('Model input shape:', shape);

          const dummy = tf.ones(shape);
          console.log('Running warmup inference...');
          const res = await loadedModel.executeAsync(dummy);

          // clear memory
          tf.dispose(res);
          tf.dispose(dummy);

          // save to both ref and state
          modelRef.current = loadedModel; // ëª¨ë¸ì„ refì— ì €ì¥
          setModel(loadedModel); // ëª¨ë¸ì„ stateì—ë„ ì €ì¥
          setLoading(1);
          console.log('Model ready');
        }
      } catch (error) {
        console.error('Error loading model:', error);
        if (isMounted) {
          modelRef.current = null; // ì—ëŸ¬ ì‹œ refë„ ì´ˆê¸°í™”
          setModel(null);
          setLoading(0);
        }
      }
    };

    // Load model immediately
    loadModel();

    return () => {
      isMounted = false;
      if (loadedModel) {
        console.log('Cleaning up model...');
        loadedModel.dispose();
      }
    };
  }, [modelName]);

  // ì—¬ê¸°ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ë©´ ë  ê²ƒ ê°™ì•„ìš”
  const sendImageToServer = async (imageData: string) => {
    // console.log('ì´ë¯¸ì§€', imageData);
    if (imageData) {
      doPredictFrame(imageData);
    }
  };

  const doPredictFrame = async (imageData: string) => {
    // refì—ì„œ ë¨¼ì € ëª¨ë¸ì„ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ stateì—ì„œ í™•ì¸
    const modelToUse = modelRef.current || model;

    if (!modelToUse) {
      console.log('Model not loaded');
      return;
    }

    tf.engine().startScope();
    try {
      // Create a temporary image element to load the screenshot
      const img = new Image();
      img.src = imageData;

      await new Promise((resolve) => {
        img.onload = resolve;
      });

      // Set canvas size to match image
      if (canvasRef.current) {
        canvasRef.current.width = img.width;
        canvasRef.current.height = img.height;
      }

      // get width and height from model's shape for resizing image
      const inputShape = modelToUse.inputs[0]?.shape; // model ëŒ€ì‹  modelToUse ì‚¬ìš©
      if (!inputShape) {
        console.log('No input shape found');
        return;
      }
      const [modelWidth, modelHeight] = inputShape.slice(1, 3);

      // pre-processing frame
      const input = tf.tidy(() => {
        const frameTensor = tf.browser.fromPixels(img);
        return tf.image
          .resizeBilinear(frameTensor, [modelWidth, modelHeight])
          .div(255.0)
          .expandDims(0);
      });

      // predicting...
      console.log('Running prediction...');
      const res = await modelToUse.executeAsync(input); // model ëŒ€ì‹  modelToUse ì‚¬ìš©
      if (!Array.isArray(res)) {
        console.log('Model output is not an array');
        return;
      }

      const [boxes, scores, classes] = res as [tf.Tensor, tf.Tensor, tf.Tensor];
      const boxesData = Array.from(boxes.dataSync());
      const scoresData = Array.from(scores.dataSync());
      const classesData = Array.from(classes.dataSync());

      // console.log('Prediction results:', {
      //   boxes: boxesData.length,
      //   scores: scoresData.length,
      //   classes: classesData.length,
      // });

      // build the predictions data
      await renderPrediction(boxesData, scoresData, classesData);

      // clear memory
      tf.dispose(res);
    } catch (error) {
      console.error('Error in prediction:', error);
    } finally {
      tf.engine().endScope();
    }
  };

  const renderPrediction = async (
    boxesData: number[],
    scoresData: number[],
    classesData: number[],
  ) => {
    if (!canvasRef.current || !webcamRef.current) return;

    const ctx = canvasRef.current.getContext('2d');
    if (!ctx) return;

    // clean canvas
    ctx.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height);

    const font = '16px sans-serif';
    ctx.font = font;
    ctx.textBaseline = 'top';

    for (let i = 0; i < scoresData.length; ++i) {
      const klass = LABELS[classesData[i]];
      const score = (scoresData[i] * 100).toFixed(1);

      // Only process if the score is above 40%
      if (parseFloat(score) < 40) continue;

      let [x1, y1, x2, y2] = boxesData.slice(i * 4, (i + 1) * 4);
      x1 *= canvasRef.current.width;
      x2 *= canvasRef.current.width;
      y1 *= canvasRef.current.height;
      y2 *= canvasRef.current.height;
      const width = x2 - x1;
      const height = y2 - y1;

      // draw the bounding box
      ctx.strokeStyle = '#C53030';
      ctx.lineWidth = 2;
      ctx.strokeRect(x1, y1, width, height);

      const label = klass + ' - ' + score + '%';
      const textWidth = ctx.measureText(label).width;
      const textHeight = parseInt(font, 10); // base 10

      // draw the label background
      ctx.fillStyle = '#C53030';
      ctx.fillRect(x1 - 1, y1 - (textHeight + 4), textWidth + 6, textHeight + 4);

      // draw the label text
      ctx.fillStyle = '#FFFFFF';
      ctx.fillText(label, x1 + 2, y1 - (textHeight + 2));

      // If bus is detected, crop and save the image
      if (klass === 'bus') {
        // Create a new canvas for the cropped image
        const cropCanvas = document.createElement('canvas');
        cropCanvas.width = width;
        cropCanvas.height = height;
        const cropCtx = cropCanvas.getContext('2d');
        if (!cropCtx) return;

        // Get the current frame from webcam
        const currentFrame = webcamRef.current.getScreenshot();
        if (!currentFrame) return;

        // Create an image from the current frame
        const frameImg = new Image();
        frameImg.src = currentFrame;

        // Wait for the image to load
        await new Promise((resolve) => {
          frameImg.onload = resolve;
        });

        // Crop the image
        cropCtx.drawImage(frameImg, x1, y1, width, height, 0, 0, width, height);

        // Convert to data URL
        const croppedImage = cropCanvas.toDataURL('image/jpeg');

        // ì´ë¯¸ì§€ ì €ì¥ ë° OCR ì²˜ë¦¬
        saveAndProcessBusImage(croppedImage);
      }
    }
  };

  // OCR API í˜¸ì¶œ í•¨ìˆ˜
  const callOCRAPI = async (imageData: string): Promise<OCRResponse> => {
    try {
      // console.log('OCR API í˜¸ì¶œ ì‹œì‘');
      const base64Data = imageData.split(',')[1];
      if (!base64Data) {
        console.error('ì´ë¯¸ì§€ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨');
        throw new Error('ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€ ë°ì´í„°ì…ë‹ˆë‹¤.');
      }

      // console.log('API ìš”ì²­ ì „ì†¡');
      const response = await fetch('/api/ocr', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          imageData: base64Data,
        }),
      });

      // console.log('API ì‘ë‹µ ìˆ˜ì‹ :', response.status);

      // ì‘ë‹µ ìƒíƒœ í™•ì¸
      if (!response.ok) {
        let errorMessage = 'API í˜¸ì¶œ ì‹¤íŒ¨';
        try {
          const errorData = await response.json();
          console.error('API ì—ëŸ¬ ì‘ë‹µ:', errorData);
          errorMessage = errorData.message || response.statusText;
        } catch (e) {
          console.error('ì—ëŸ¬ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:', e);
          errorMessage = response.statusText;
        }
        throw new Error(errorMessage);
      }

      // ì„±ê³µ ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±
      let result;
      try {
        result = (await response.json()) as OCRResponse;
        // console.log('OCR ê²°ê³¼ ìˆ˜ì‹ :', result);
      } catch (e) {
        console.error('ì‘ë‹µ íŒŒì‹± ì—ëŸ¬:', e);
        throw new Error('API ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
      }

      // OCR ê²°ê³¼ ê²€ì¦
      if (!result.images || !result.images[0] || !result.images[0].fields) {
        console.error('ì˜ëª»ëœ OCR ê²°ê³¼ í˜•ì‹:', result);
        throw new Error('OCR ê²°ê³¼ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.');
      }

      return result;
    } catch (error) {
      console.error('OCR API ì—ëŸ¬:', error);
      if (error instanceof Error) {
        console.error('ì—ëŸ¬ ìƒì„¸:', {
          message: error.message,
          stack: error.stack,
        });
      }
      throw error;
    }
  };

  // ì§„ì§œ 3ë°•ì ë§¤ì¹­ í•¨ìˆ˜ - OCR + APIë°°ì—´ + ì…ë ¥í•œë²„ìŠ¤ë²ˆí˜¸
  const checkBusMatch = (detectedBusNumber: string) => {
    console.log('=== 3ë°•ì ë²„ìŠ¤ ë§¤ì¹­ ì²´í¬ ì‹œì‘ ===');
    console.log(`OCR ê°ì§€ëœ ë²„ìŠ¤: "${detectedBusNumber}"`);

    // ì¡°ê±´ 1: ì‚¬ìš©ìê°€ ì…ë ¥í•´ë‘” ë²„ìŠ¤ ë²ˆí˜¸ê°€ ìˆì–´ì•¼ í•¨
    const userInputBusNumber = getBusNumber();
    console.log(`ì‚¬ìš©ì ì…ë ¥ ë²„ìŠ¤ ë²ˆí˜¸: "${userInputBusNumber}"`);

    if (!userInputBusNumber) {
      console.log('âŒ ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë²„ìŠ¤ ë²ˆí˜¸ê°€ ì—†ìŠµë‹ˆë‹¤');
      return false;
    }

    // ì¡°ê±´ 2: isRegisteredBusArrivingì´ trueì—¬ì•¼ í•¨ (ë“±ë¡í•œ ë²„ìŠ¤ê°€ ì‹¤ì œ ë„ì°© ì˜ˆì •)
    console.log(`ë“±ë¡ ë²„ìŠ¤ ë„ì°© ì˜ˆì • ìƒíƒœ: ${isRegisteredBusArriving}`);

    if (!isRegisteredBusArriving) {
      console.log('âŒ ë“±ë¡í•œ ë²„ìŠ¤ê°€ í˜„ì¬ ë„ì°© ì˜ˆì •ì´ ì•„ë‹™ë‹ˆë‹¤');
      return false;
    }

    // ì¡°ê±´ 3: APIì—ì„œ ë°›ì€ ë„ì°© ì˜ˆì • ë²„ìŠ¤ ë°°ì—´ì— í•´ë‹¹ ë²„ìŠ¤ê°€ ìˆì–´ì•¼ í•¨
    console.log(`API ë„ì°© ì˜ˆì • ë²„ìŠ¤ ëª©ë¡:`, expectedBuses);

    if (!expectedBuses || expectedBuses.length === 0) {
      console.log('âŒ API ë„ì°© ì˜ˆì • ë²„ìŠ¤ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤');
      return false;
    }

    const detectedStr = String(detectedBusNumber).trim();
    const userInputStr = String(userInputBusNumber).trim();

    // ì¡°ê±´ 4: OCR ê²°ê³¼ === ì‚¬ìš©ì ì…ë ¥ ë²„ìŠ¤ ë²ˆí˜¸
    console.log(`OCR "${detectedStr}" vs ì‚¬ìš©ìì…ë ¥ "${userInputStr}"`);

    if (detectedStr !== userInputStr) {
      console.log(`âŒ OCR ê²°ê³¼(${detectedStr})ì™€ ì‚¬ìš©ì ì…ë ¥(${userInputStr})ì´ ë‹¤ë¦…ë‹ˆë‹¤`);
      return false;
    }

    // ì¡°ê±´ 5: API ë°°ì—´ì—ë„ í•´ë‹¹ ë²„ìŠ¤ê°€ ìˆì–´ì•¼ í•¨
    const isInApiList = expectedBuses.some((bus) => String(bus.busNumber).trim() === detectedStr);

    console.log(`API ë°°ì—´ì— ${detectedStr} ì¡´ì¬ ì—¬ë¶€: ${isInApiList}`);

    if (!isInApiList) {
      console.log(`âŒ API ë°°ì—´ì— ${detectedStr}ë²ˆ ë²„ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤`);
      console.log(`API ë°°ì—´ ë²„ìŠ¤ë“¤: [${expectedBuses.map((b) => b.busNumber).join(', ')}]`);
      return false;
    }

    console.log(`ğŸ‰ğŸ‰ğŸ‰ 3ë°•ì ëª¨ë‘ ì¼ì¹˜! ì™„ë²½í•œ ë§¤ì¹­!`);
    console.log(`âœ… OCR ê°ì§€: "${detectedStr}"`);
    console.log(`âœ… ì‚¬ìš©ì ì…ë ¥: "${userInputStr}"`);
    console.log(`âœ… API ë°°ì—´ì— ì¡´ì¬: ${isInApiList}`);
    console.log(`âœ… ë“±ë¡ ë²„ìŠ¤ ë„ì°© ì˜ˆì •: ${isRegisteredBusArriving}`);
    console.log('=== 3ë°•ì ë§¤ì¹­ ì™„ë£Œ ===');
    return true;
  };

  // ìˆ˜ì •ëœ ë²„ìŠ¤ ì´ë¯¸ì§€ ì €ì¥ ë° OCR ì²˜ë¦¬
  const saveAndProcessBusImage = async (croppedImage: string) => {
    try {
      console.log('ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘');
      const ocrResult = await callOCRAPI(croppedImage);

      if (ocrResult) {
        const busNumber = extractBusNumber(ocrResult);
        if (busNumber) {
          console.log('ğŸ¯ ë²„ìŠ¤ ë²ˆí˜¸ ì¸ì‹ ì„±ê³µ:', busNumber);
          setDetectedBus(busNumber);

          console.log('ğŸ  hasNearbyStops ì²´í¬:', hasNearbyStops);
          console.log('ğŸšŒ expectedBuses:', expectedBuses);
          console.log('âœ… isRegisteredBusArriving:', isRegisteredBusArriving);
          console.log('ğŸ“Š ì „ì²´ busArrivalData:', busArrivalData);

          // ê·¼ì²˜ì— ì •ë¥˜ì¥ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ë§¤ì¹­ ì²´í¬
          if (hasNearbyStops) {
            console.log('âœ… ì •ë¥˜ì¥ì´ ìˆìŒ - ë§¤ì¹­ í•¨ìˆ˜ í˜¸ì¶œ');
            // 3ë°•ì ëª¨ë‘ ì²´í¬í•˜ëŠ” í•¨ìˆ˜ í˜¸ì¶œ
            const isMatching = checkBusMatch(busNumber);
            console.log(`ğŸ” ë§¤ì¹­ ê²°ê³¼: ${isMatching}`);
            setIsDetectedBusArriving(isMatching);

            if (isMatching) {
              console.log('ğŸ‰ ì•Œë¦¼ í‘œì‹œ!');
              setShowNotification(true);
              setTimeout(() => setShowNotification(false), 5000);
            }
          } else {
            // ê·¼ì²˜ì— ì •ë¥˜ì¥ì´ ì—†ìœ¼ë©´ ë§¤ì¹­í•˜ì§€ ì•ŠìŒ
            console.log('âŒ ê·¼ì²˜ì— ì •ë¥˜ì¥ì´ ì—†ì–´ ë§¤ì¹­í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤');
            setIsDetectedBusArriving(false);
          }
        } else {
          console.log('âŒ ë²„ìŠ¤ ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ', ocrResult.images[0].fields);
        }
      }
    } catch (error) {
      console.error('ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬:', error);
      if (error instanceof Error) {
        console.error('ì—ëŸ¬ ìƒì„¸:', {
          message: error.message,
          stack: error.stack,
        });
      }
    }
  };

  // ë²„ìŠ¤ ë²ˆí˜¸ ì¶”ì¶œ í•¨ìˆ˜
  const extractBusNumber = (ocrResult: OCRResponse): string | null => {
    try {
      const fields = ocrResult.images[0].fields;
      if (!fields || !fields.length) return null;
      console.log('fields ì „ì²´ ë‚´ìš©: ', fields);
      //ë²„ìŠ¤ ë²ˆí˜¸ íŒ¨í„´
      const busNumberPatterns = [
        /^\d{1,4}[-\s]?\d{1,4}$/, // ì¼ë°˜ ë²„ìŠ¤ (1, 1234-5678)
        /^[ê°€-í£]\d{1,4}$/, // ë§ˆì„ë²„ìŠ¤ (ê°•ë‚¨1)
        /^[A-Z]\d{1,4}$/, // ê³µí•­ë²„ìŠ¤ (A1)
        /^[ê°€-í£]\d{1,4}[-\s]?\d{1,4}$/, // ì§€ì„ ë²„ìŠ¤ (ê°•ë‚¨1-1234) ë” ìˆìœ¼ë©´ ì¶”í›„ ì¶”ê°€
      ];

      for (const field of fields) {
        const text = field.inferText.replace(/\s/g, '');
        for (const pattern of busNumberPatterns) {
          if (pattern.test(text)) {
            return text;
          }
        }
      }

      return null;
    } catch (error) {
      console.error('ë²„ìŠ¤ ë²ˆí˜¸ ì¶”ì¶œ ì¤‘ ì—ëŸ¬:', error);
      return null;
    }
  };

  const capture = useCallback(() => {
    const imageSrc = webcamRef.current?.getScreenshot();
    if (imageSrc) {
      sendImageToServer(imageSrc);
    }
  }, []);

  const continuousCapture = useCallback(() => {
    if (captureInterval.current) {
      clearInterval(captureInterval.current);
    }

    captureInterval.current = setInterval(() => {
      capture();
    }, 1000);
  }, [capture]);

  useEffect(() => {
    requestCameraPermission();
    return () => {
      if (captureInterval.current) {
        clearInterval(captureInterval.current);
      }
    };
  }, []);

  useEffect(() => {
    if (hasPermission === true) {
      continuousCapture();
    }
  }, [hasPermission, continuousCapture]);

  // ëª¨ë¸ ë¡œë”© ìƒíƒœì— ë”°ë¥¸ UI ì²˜ë¦¬
  if (loading < 1) {
    return (
      <div className="flex h-screen flex-col items-center justify-center">
        <div className="text-center">
          <p className="mb-4 text-lg font-medium">ëª¨ë¸ ë¡œë”© ì¤‘...</p>
          <div className="w-64 rounded-full bg-gray-200">
            <div
              className="h-2 rounded-full bg-blue-500 transition-all duration-300"
              style={{ width: `${loading * 100}%` }}
            />
          </div>
          <p className="mt-2 text-sm text-gray-600">{Math.round(loading * 100)}%</p>
        </div>
      </div>
    );
  }

  if (hasPermission === null) {
    return (
      <div className="flex h-screen items-center justify-center">
        ì¹´ë©”ë¼ ê¶Œí•œì„ ìš”ì²­ ì¤‘ì…ë‹ˆë‹¤...
      </div>
    );
  }

  if (hasPermission === false) {
    return (
      <div className="flex h-screen flex-col items-center justify-center p-4 text-center">
        <p className="mb-4 text-red-500">ì¹´ë©”ë¼ ì ‘ê·¼ ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤.</p>

        <button
          onClick={requestCameraPermission}
          className="rounded-lg bg-blue-500 px-6 py-3 font-medium text-white transition-colors hover:bg-blue-600"
        >
          ì¹´ë©”ë¼ ê¶Œí•œ ë‹¤ì‹œ ìš”ì²­í•˜ê¸°
        </button>
      </div>
    );
  }

  return (
    <div className="flex h-screen flex-col bg-white">
      <div className="relative aspect-[3/4] max-h-[60vh] w-full overflow-hidden">
        <Webcam
          audio={false}
          ref={webcamRef}
          screenshotFormat="image/jpeg"
          videoConstraints={{
            facingMode: 'environment',
            aspectRatio: 4 / 3,
          }}
          className="h-full w-full object-cover"
          screenshotQuality={1}
        />
        <canvas
          ref={canvasRef}
          className="absolute top-0 left-0 h-full w-full"
          style={{ zIndex: 1 }}
        />

        {/* ìˆ˜ì •ëœ Bus Arrival Notification */}
        {showNotification && (
          <div
            className="absolute top-4 right-0 left-0 mx-auto w-4/5 rounded-lg bg-green-500 p-4 text-center text-white shadow-lg"
            style={{ zIndex: 2 }}
          >
            <p className="text-lg font-bold">ë“±ë¡í•œ ë²„ìŠ¤ê°€ ë„ì°©í–ˆìŠµë‹ˆë‹¤!</p>
            <p>{detectedBus}ë²ˆ ë²„ìŠ¤ê°€ ê³§ ë„ì°©í•©ë‹ˆë‹¤</p>
          </div>
        )}
      </div>

      <div className="mt-4 p-4">
        {detectedBus && (
          <div
            className={`mb-4 rounded-full p-4 text-center ${
              isDetectedBusArriving ? 'bg-[#ffd700]' : 'bg-gray-100'
            }`}
          >
            <p className="text-7xl font-bold text-[#353535]">{detectedBus}</p>
          </div>
        )}

        {/* Expected bus arrivals */}
        <div className="mt-2 mb-4">
          <p className="mb-2 font-medium">ë„ì°© ì˜ˆì • ë²„ìŠ¤</p>
          {hasNearbyStops ? (
            expectedBuses.length > 0 ? (
              <div>
                <div className="mb-2 flex flex-wrap gap-2">
                  {expectedBuses.map((bus, index) => (
                    <span
                      key={index}
                      className="rounded-full bg-[#ffd700] px-3 py-1 text-sm font-semibold text-[#353535]"
                    >
                      {bus.busNumber}
                    </span>
                  ))}
                </div>
                {isRegisteredBusArriving ? (
                  <p className="text-sm font-medium text-green-600">
                    âœ… ë“±ë¡í•œ ë²„ìŠ¤ê°€ ë„ì°© ì˜ˆì •ì…ë‹ˆë‹¤!
                  </p>
                ) : (
                  <p className="text-sm text-gray-500">ë“±ë¡í•œ ë²„ìŠ¤ëŠ” í˜„ì¬ ë„ì°© ì˜ˆì •ì´ ì•„ë‹™ë‹ˆë‹¤</p>
                )}
              </div>
            ) : (
              <p className="text-gray-500">ë„ì°© ì˜ˆì • ë²„ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤</p>
            )
          ) : (
            <p className="text-orange-500">ê·¼ì²˜ì— ë²„ìŠ¤ ì •ë¥˜ì¥ì´ ì—†ìŠµë‹ˆë‹¤</p>
          )}
        </div>

        {/* ë””ë²„ê¹…ìš© ì •ë³´ í‘œì‹œ (ê°œë°œ ì¤‘ì—ë§Œ ì‚¬ìš©) */}
        <div className="mt-4 rounded bg-gray-100 p-2 text-xs text-gray-600">
          <p>ğŸ“± OCR ê°ì§€ ë²„ìŠ¤: {detectedBus || 'ì—†ìŒ'}</p>
          <p>ğŸ‘¤ ì‚¬ìš©ì ì…ë ¥ ë²„ìŠ¤: {getBusNumber() || 'ì—†ìŒ'}</p>
          <p>ğŸšŒ API ë„ì°©ì˜ˆì • ë²„ìŠ¤: {expectedBuses.map((b) => b.busNumber).join(', ') || 'ì—†ìŒ'}</p>
          <p>ğŸ  ê·¼ì²˜ ì •ë¥˜ì¥: {hasNearbyStops ? 'ìˆìŒ' : 'ì—†ìŒ'}</p>
          <p>âœ… ë“±ë¡ ë²„ìŠ¤ ë„ì°© ì˜ˆì •: {isRegisteredBusArriving ? 'ì˜ˆ' : 'ì•„ë‹ˆì˜¤'}</p>
          <p>ğŸ¯ ìµœì¢… ë§¤ì¹­: {isDetectedBusArriving ? 'ì„±ê³µ' : 'ì‹¤íŒ¨'}</p>
        </div>
      </div>
    </div>
  );
}
